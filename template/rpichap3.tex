%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER THREE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter{Methods} \label{sec:methods}

In this chapter, we detail the finalized algorithm pipeline, as well as some alternative approaches that we found to have limited successs.

\section{Trailing Edge Extraction}

Extracting good, high quality trailing edges images is one of the primary challenges when matching Humpback whales by their trailing edge.
In this section, we detail the steps that go into automating the extraction of high quality trailing edges, while trying to minimize human intervention.

One major assumption that we make when extracting these trailing edges is that the Humpback whale fluke is aligned such that its major axis is horizontal.
Additionally, we generally assume that all parts of the fluke are present, however theoretically it would be possible to match a partial fluke -- although unlikely.

While these assumptions do not make for an incredibly robust system, the nature of the problem (and the dataset that we had at hand) makes these assumptions reasonable.

\subsection{Basic Contour Extraction Algorithm}

The base algorithm that is used for extracting the trailing edge uses the vertical gradient information of the image (denoted as $I_y$).
We extract $I_y$ using a vertically oriented $5 \times 5$ Sobel kernel\cite{Sobel1968}.  

We then normalize $I_y$ with min-max scaling, giving $N_y$ as

\begin{equation} \label{norm01}
N_{y} = \frac{I_y - min(I_y)}{max(I_y) - min(I_y)}
\end{equation}

With $N_y$, we then need a starting point and ending point for the algorithm, denoted $s$ and $e$ respectively. 
For our purposes, we use the left and right tips of the fluke as our start and end.
We explain how these points are determined in a later section.
We then take the columns corresponding to these points and set every pixel in $N_y(s_x,\cdot)$ (i.e. the column $s_x$) and $N_y(e_x,\cdot)$ to $\infty$,  and then set those points in $N_y$ to 0. 
This forces the path to start and end at these points (as we are finding the minimal path between them).

The minimal path is then found by scanning the columns of $N_y$ from left to right, starting and ending at $s$ and $e$ respectively. 
For each pixel in a column we set its cost with the update rule

\begin{equation} \label{te_update}
C(x,y) = min_{y_c=(y-n)}^{y+n}(C(x-1, y_c) + N_y(x,y))
\end{equation}

Where $n$ is a neighborhood constraint which we default to $2$, meaning that each pixel considers $5$ 'neighbors' in the previous column.

If $y_c$ is out of bounds, we let $C(x-1, y_c) = \infty$, and if $x - 1 < 0$, we let $C(x-1, \cdot) = 0$.

As $C$ is filled out, we also keep a backtrace matrix $B$, which keeps track of the index of the minimal candidate chosen in equation 3.2.
Once the end column is reached, we work backwards from $e$ to construct the path, adding the coordinate corresponding to $B(x,y)$ at each step.

% TODO: Put in actual algorithm pseudocode

We can also extend this algorithm by adding extra 'control points' which the path is forced through, using the same methodology as forcing the path through the start and end points.
Commonly, we use the bottom of the notch as a control point, although this affects accuracy negatively if it is inaccurate.

While this algorithm has no understanding of Humpback whale flukes, a lot of images that are constrained around the Fluke with oceanic backgrounds (which is a large majority of the dataset at hand) provide high quality trailing edges when put through this algorithm.
However, this is (obviously) not a robust algorithm for finding trailing edges, something that we will attempt to fix later on.

\subsection{Fluke keypoint prediction}

One major issue with automating the above trailing edge extraction algorithm is that it requires manual annotation in the selection of the starting and ending points (i.e., the fluke keypoints), as well as any control points (specifically the bottom of the notch).
To work around this, we propose a convolutional network that predicts these tip points as part of the identification pipeline.

The convolutional network does not need full-sized images, so the first step of the keypoint extraction pipeline is to resize the image to $256 \times 256$ pixels.
This size choice is somewhat arbitrary, but we find that it provides the best performance without using an unnecessary amount of memory.
The network then predicts three points (left tip, right tip, and the bottom of the notch) in the range $[0-1]$ for both $x$ and $y$. 
This is then rescaled back up to the original image size.
%The evaluated network architectures are detailed in Table 3.1. % TODO put this in

\subsubsection{Network Design}

The overall design of the network follows the pattern of alternating small ($3 \times 3$) convolutional filters with $2 \times 2$ max pooling layers, at each step doubling the number of channels (starting with $8$ channels).
This is somewhat similar to VGG-16, although with half the trainable layers.
After a $32\times$ downsample has been achieved, we attach a decision layer which consists of a dense layer followed by three separate dense layers with separate predictions layers after (one for each point being predicted).
While this is not a common approach in keypoint prediction, we found that it gave better performance than having the points predicted as a single vector.

We theorize that this may be because shared units between each of the three predictions leads to stronger correlations between them, reducing overall prediction flexibility.

\subsubsection{Training Details}

Generating the training data for this is straightforward given a set of annotations with the associated points to learn.
% TODO: Once we've cleared up what the datasets are, put in the details of how the training / val / test sets were made
The dataset that we created for this purpose contains approximately $2700$ training images, $900$ validation images, and $1200$ test images.
Despite the small size of the training set, we found that it generalizes pretty well.

First, each image is resized to a fixed width while maintaining the aspect ratio.
This is done to somewhat normalize the relative scale of objects in each image on the assumption that they are constrained to contain the fluke.
Each image is rescaled to the network size, and then the corresponding targets are rescaled to the range $[0-1]$.
The size of the original image is recorded as well.
While it would be possible to treat this as a simple multi-variate regression and use RMSE loss, we achieved better results by averaging the Euclidean distance between the predicted points and true points.
We also include a scalar scaling factor $\alpha$, which scales each point by a proportion of the original image size.
Thus, we have the scaled Euclidean loss $SE$

\begin{equation}
SE(\vec{t}, \vec{p}, \vec{s}, \alpha) = \lVert (\alpha * \vec{s}) \odot (\vec{t} - \vec{p}) \rVert
\end{equation}
Where
\begin{itemize}
    \item $\vec{t}$ and $\vec{p}$ are the $(x, y)$ ground truth and predicted values respectively
    \item $\vec{s}$ is the original image width and height
\end{itemize}

The networks are trained for 100 epochs with $\alpha$ set to \num{2e-2}, using Nesterov momentum with a learning rate of \num{1e-1} and $l2$ regularization on the trainable parameters with a decay of \num{1e-4}.
All of these hyper parameters were tuned using the validation set, although the possible parameter space was not fully explored due to time constraints.

\subsubsection{Evaluation}

%TODO Put in a bunch of figures and talk about this
% Problem is recreating all these experiments / networks for evaluation :/
% Might take a while...

On average, the best network achieved a 10 pixel distance error on the validation and testing sets (in the original image scale).
While this may seem like a lot, the trailing edge extraction (and subsequent matching performance) was not severely affected when only using the start and end point predictions.

%Additionally, as can be seen in Figure 3.2, 
We find that, for the vast majority of images, the network achieves a low pixel distance error, while there are a few that have a much higher error.
Qualitative inspection of these images shows that they are of flukes which are not the singular or major object in the image, nor horizontally oriented.

We attempted to use a spatial transformer network\cite{jaderberg2015spatial} to try and handle these cases, but we were unable to get it to perform as well as the standard convolutional network, nor produce sensible transformations.

\subsection{Trailing Edge Scoring}

As mentioned in the beginning of this section, using only the gradient information for extracting the trailing edge works in a lot of cases, but is not a robust method.
% TODO Figures demonstrating this

If we had a score of each pixel's "trailing edginess" in an image, the trailing edge extractor could make use of this information to make better choices in trailing edge extraction.
To do this, we need a prediction of whether or not each pixel belongs to the trailing edge of a fluke, a task that is best suited to a fully convolutional network.
In these networks, all convolutions (aside from max pooling layers) are "same" convolutions, which have square, odd filters (usually $3 \times 3$) and 1-padding.
These "same" convolutions produce a spatial output shape that is the same as the input shape, obviating the need for any interpolation.

The four major variants on trailing edge scoring networks that we evaluated are detailed below.
All of these networks function on the same paradigm of taking an arbitrarily sized image and producing an image of the same size but with a class score for each pixel.

The dataset was sourced from existing trailing edges extracted using the gradient based algorithm, however with manual adjustments to fix many of the more common issues we encountered with this algorithm.

To generate the training set, we then extract a $128 \times 128$ patch at $128$ pixel intervals along the trailing edge (a positive patch), and a corresponding negative patch (with no trailing edge pixels) randomly sampled from the left over space above and below the trailing edge.
These patches are then randomly split into training, testing, and validation sets each with $3700$, $1200$, and $1600$ patches respectively.

\subsubsection{Trailing Edge Scoring Architectures}

There are several network architectures that were tried, however here we will report only the major variants.
One major consideration that has to be made when selecting a fully convolutional network architecture is its receptive field.
If the receptive field is too small, it may not have enough information to accurately determine if a given pixel is part of the trailing edge.
However, in order to increase the receptive field without massively increasing the depth of a network, we must 

\paragraph{Simple}
This network is simply a stack of $6$ "same" convolutional layers, with no downsampling regions.
This has a small receptive field, but can produce detailed predictions.
Due to the small receptive field however, at convergence it gives low precision predictions, and seems to (in many cases) produce a lot of the same mistakes that normal gradient based trailing edges do.
% TODO include this architecture description in a table somewhere
% TODO include image showing trailing edge prediction from Simple network

\paragraph{Upsample}
To deal with the small receptive fields, convolutional networks typically downsample at various stages. 
This downsampling usually takes the form of max pooling (instead of e.g. convolutional kernels with a stride greater than one).
The upsample architecture is analogous to the FCN-32s architecture in \cite{long2015fully}, although we do not go down to a 1x1 spatial extent before upsampling.
This produces blocky and nearly unusable trailing edge scores, however they tend to be more connected than the disparate (but fine-grained) predictions made by the simple network.

% TODO same as simple

\paragraph{Jet}
Following the deep-jet architecture from \cite{long2015fully}, we combine softmax predictions from various stages throughout the network with predictions from "farther down" the network, cascading until we hit predictions from the last convolutional layer before any downsampling.
This gives more fine-grained predictions than the upsample network while giving somewhat better predictions than the simple network.

% TODO same as simple

\paragraph{DeepRes}
While the receptive field of the Simple network is small, it can produce very fine trailing edges, which we found to be necessary for good trailing edge extraction.
However with such a small receptive field it can be more prone to making mistakes.
It is possible to create a very deep network of 'same' convolutions, however training very deep networks like this can run into problems very quickly.
Recently, there has been work on making very deep networks feasible by adding residual connections\cite{he2015deep}, showing that these very deep networks can achieve high accuracy in a challenging benchmark.
We replicate this architecture by stacking $64$ $3\times3$ 'same' convolution kernels, and adding a residual connection every other layer.
This performs better than the Simple network, however we do not see a marked improvement in trailing edge matching as a result.

\subsubsection{Using the trailing edge scores}

Once we have 'trailing edginess' score for each pixel, we need to combine this information with the gradient in a way that causes the trailing edge extraction algorithm to follow those pixels that the scoring map marks as trailing edge.
More formally, the trailing edge scoring map gives us an image $T_p \in [0-1]^{w \times h}$ which denotes the network's predicted probability of each pixel being part of the trailing edge.
The most simple and obvious way to combine this information with $N_y$ from before is to combine them with some weight $\beta$.

\begin{equation}
S_{te} = (1 - \beta)*N_y + \beta*(1 - T_p)
\end{equation}

We use $1 - T_p$ in this case because we are minimizing the path through $S_{te}$.
Once done, the trailing edge extraction algorithm procedes as described in the previous section.

Another variant on combining $T_p$ and $N_y$ that we tried was to dilate the trailing edge predictions and then forbid the trailing edge from going outside of those predictions.
However one of the main issues with this is that if there are any breaks in the prediction (which can happen even with the Upsample architecture), the trailing edge cannot be extracted, leading to broken trailing edges.

\section{Trailing Edge Matching}

Once these trailing edges are extracted, we can use a few simple methods to compare them in search of a trailing edge that is close to the one we are finding a match for.
The simplest way to do this is to define a comparator between any two given trailing edges.
The best method for this that we have tried is to extract block integral curvature at several scales and then use Euclidean distance as a point-to-point distance measure in a dynamic time warping algorithm.
Block integral curvature approximates true integral curvature while allowing for a much faster computation time.
This approximation is made better by being taken at multiple scales.

\subsection{Curvature Measurement}

To extract the curvature, we take the trailing edge (which is a set of coordinates into an image) and a zero-image $I_0$.
Each pixel corresponding to and below (in the vertical direction) the trailing edge in $I_0$ are set to $1$.
Because the trailing edge extractor produces only one coordinate per column, we can do this safely.
Once this is done, we calculate a summed area table \cite{crow1984summed} $ST$ from $I_0$ as follows.

\begin{equation}
ST(x,y) = \sum_{i=0}^{i=y}\sum_{j=0}^{j=x} I_0 
\end{equation}

Conceptually, the next step is to slide a square of size $s$ centered on each point, and measure the percentage of that square that is within the filled in trailing edge.
To do this, we compute $BC_s(x, y)$ for each $(x, y)$ coordinate in the trailing edge.

\begin{align}
b_x &= x - \frac{s}{2}\\
b_y &= y - \frac{s}{2}\\
e_x &= x + \frac{s}{2}\\
e_y &= y + \frac{s}{2}\\
BC_s(x,y) &= \frac{(ST(b_x, b_y) + ST(e_x, e_y)) - (ST(b_x, e_y) + ST(e_x, b_y))}{s^2}
\end{align}

The set of scales to choose presents a large parameter space, however we have found that the scales $S = [5, 10, 15, 20]$ work well for our purposes.
We then treat this curvature measurement as a $\|S\| \times x$ matrix $BC$.

\subsection{Sequence Matching}

Given two sequences of curvatures $BC_1$ and $BC_2$, we match them using dynamic time warping as follows.
First, we create a cost matrix $C$ of size $x_1 \times x_2$ (where $x$ is the number of points in the original trailing edge).
We initialize this cost matrix by setting the first column and row to $\infty$, and then $C(0,0) = 0$.
Then, for each element in the cost matrix starting with $C(1,1)$, we use the following update rule

\begin{align}
CD_{cw}(c_1, c_2) &= \lVert cw \odot (\vec{c_1} - \vec{c_2}) \rVert_2\\
C(i,j) &= CD(x_1(i,\cdot),x_2(j,\cdot)) + min(C(i-1,j), C(i,j-1), C(i-1, j-1))
\end{align}

Where $cw$ is the weight given to each curvature scale.

Additionally, we impose the Sakoe-Chiba \cite{sakoe1978dynamic} locality constraint $w$ so that for each element $i$ in $BC_1$, we only consider the range over elements $j$ in $BC_2$ of $j \in [min(i - w, 0), max(i + w, x_2)]$.

For most of these experiments $w$ is set to $50$, which appears to minimize the time taken for each comparison while preserving the overall accuracy of the algorithm.

\section{Alternative Approaches}

In this section, we list and briefly describe alternative approaches that were tried, although they did not prove accurate enough to make it into the final system.

\subsection{Aligning Trailing Edges}

One obvious pre-processing step that would make sense when comparing trailing edges is to make sure that they are aligned in image space.
However, we found that doing so when comparing curvature was often unnecessary (due to the invariances to rotation and translation, scale was taken care of separately), and using the Euclidean distance between points on the (aligned) trailing edges did not give good accuracies.

\subsubsection{Keypoint Alignment}

As noted in the section on fluke keypoints, there are three points to predict -- left, notch and right.
Originally the intention for recording these was to have three corresponding points with which to align images before extracting trailing edge.
This was done by computing the inverse affine transform that maps the database image onto the query image, and then computing the curvature information.

One major issue with aligning these trailing edges however is making the distance invariant to skew, which is unfortunately not something that can be easily fixed with an affine transformation.

\subsubsection{Dynamic Time Warping Alignment}

Taking after the AI-DTW approach laid out in \cite{qiao2006affine}, we ran an iterative alignment process that used the correspondences found by DTW (using either curvature distance or Euclidean distance as criteria).
However, we found that this process oftentimes wouldn't converge, and when it did the alignments provided were of worse quality than those found by aligning the three fluke keypoints.

\subsection{Histogram Matching}

One early curvature comparison method that was tried was to use histograms to match instead of a sequence-based method.
This is the usual approach for comparing curvatures \cite{kumar2012leafsnap}, %TODO cite more for this
however we found that for our purposes even high resolution histograms did not provide enough detail to match the humpback flukes, as they were usually dominated in one area.

\subsection{Embedding via Convolutional Networks}

We also made an attempt at training convolutional networks to directly embed the images of flukes into a $n$ dimensional Euclidean space, much like \cite{schroff2015facenet} and \cite{parkhi2015deep}.
However, most of the previous literature on this technique is applied to larger datasets such as LFW %TODO find citation for LFW
, which is significantly larger than the dataset that was made available to us.
Additionally, these larger datasets often have five to ten images per identity (if not more), whereas most of the identities in our dataset had one or two images associated.
While this method was attempted, even a severely overfit convolutional network only achieved half the top-1 accuracy on its training set that the main method is able to achieve.
We tried both triplet loss (a modified version of the one detailed in \cite{schroff2015facenet}) and contrastive loss %TODO find citation for this?
to no avail.

We believe that the small amount of images per identity is the main factor for the failure of these methods, and that a larger dataset would be necessary to properly train them.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
