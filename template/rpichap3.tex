%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER THREE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter{Methods} \label{sec:methods}

In this chapter, we detail the finalized algorithm pipeline, as well as some alternative approaches that we found to have limited successs.

\section{Trailing Edge Extraction}

Extracting good, high quality trailing edges images is one of the primary challenges when matching Humpback whales by their trailing edge.
In this section, we detail the steps that go into automating the extraction of high quality trailing edges, while trying to minimize human intervention.

\subsection{Basic Contour Extraction Algorithm}

The base algorithm that is used for extracting the trailing edge needs the vertical gradient information of the image (denoted as $I_y$).
We extract $I_y$ using a vertically oriented $5 \times 5$ Sobel kernel. % TODO: Either find a good citation for this or put the actual kernel in

We then normalize $I_y$ with min-max scaling, giving $N_y$ as

\begin{equation} \label{norm01}
N_{y} = \frac{I_y - min(I_y)}{max(I_y) - min(I_y)}
\end{equation}

With $N_y$, we then need a starting point and ending point for the algorithm, denoted $s$ and $e$ respectively. 
For our purposes, we use the left and right tips of the fluke as our start and end.
We then take the columns corresponding to these points and set every pixel in $N_y(s_x,\cdot)$ (i.e. the column $s_x$) and $N_y(e_x,\cdot)$ to $\infty$,  and then set those points in $N_y$ to 0. 
This forces the path to start and end at these points (as we are finding the minimal path).

The minimal path is then found by, scanning each column from left to right, and for each pixel in a column we set its cost with the update rule
\begin{equation} \label{te_update}
C(x,y) = min_{y_c=(y-n)}^{y+n}(C(x-1, y_c) + N_y(x,y))
\end{equation}

Where $n$ is a neighborhood constraint which we default to 2, meaning that each pixel considers 5 'neighbors' in the previous column.
If $y_c$ is out of bounds, then we have $C(x-1, y_c) = \infty$, and if $x - 1 < 0$, we set $C(x-1, \cdot)$ to $0$.

As $C$ is filled out, we also keep a backtrace matrix $B$, which keeps track of the index of the minimal candidate chosen in equation 3.2.
Once the end column is reached, we work backwards from $e$ to construct the path, adding the coordinate corresponding to $B(x,y)$ at each step.

% TODO: Put in actual algorithm pseudocode

We can also extend this algorithm by adding extra 'control points' which the path is forced through, using the same methodology as forcing the path through the start and end points.
When using manual annotations in evaluating this algorithm, we use the bottom of the notch as a 'control point' as well.

While this algorithm has no understanding of Humpback whale flukes, a lot of images that are constrained around the Fluke with oceanic backgrounds (which is a large majority of the dataset at hand) provide high quality trailing edges when put through this algorithm.
However, this is (obviously) not a robust algorithm for finding trailing edges, something that we will fix later on.

\subsection{Fluke keypoint prediction}

One major issue with automation and this algorithm is that it requires manual annotation in the selection of the starting and ending points (i.e., the fluke keypoints).
To work around this, we propose a convolutional network that predcits these tip points as part of this identification pipeline.

The convolutional network does not need full-sized images, so the first step of the keypoint extraction pipeline is to resize the image to $128 \times 128$.
This size choice is somewhat arbitrary, but it provides the best performance without using an unnecessary amount of GPU memory.
The network then predicts each of three points (left, right, and notch point -- the last of which is optional to the trailing edge extraction) in the range $[0-1]$ for both $x$ and $y$. 
This is then rescaled back up to the original image size.
The evaluated network architectures are detailed in Table 3.1. % TODO put this in

\subsubsection{Network Design}

The overall design of the network follows the pattern of alternating small ($3 \times 3$) convolutional filters with $2 \times 2$ max pooling layers, at each step doubling the number of channels.
This is somewhat similar to VGG-16, although with half the trainable layers.
After a 32x downsample has been achieved, there are two dense layers with a small number of units.
One major difference in this work is that for predicting three points, we find that decoupling the final dense layer into three separate (smaller) layers (one for each point to predict) gives better performance than predicting them all from one final dense layer.
We theorize that this may be because shared biases between each of the three predictions leads to stronger correlations between them, reducing overall flexibility and performance.

\subsubsection{Training Details}

Generating the training data for this is straightforward given a set of annotations with the associated points to learn.
% TODO: Once we've cleared up what the datasets are, put in the details of how the training / val / test sets were made
First, each image is resized to a fixed width while maintaining the aspect ratio.
This is done to somewhat normalize the relative scale of objects in each image on the assumption that they are constrained to contain the fluke.
Each image is rescaled to the network size ($128 \times 128$ as above), and then the corresponding targets are rescaled to the range $[0-1]$.
The size of the original image is recorded as well.
While it would be possible to treat this as a simple multi-variate regression and use RMSE loss, we achieved better results by averaging the Euclidean distance between the predicted points and true points.
We also include a scalar scaling factor $\alpha$, which scales each point by a proportion of the original image size.
Thus, we have the scaled Euclidean loss $SE$

\begin{equation}
SE(\vec{t}, \vec{p}, \vec{s}, \alpha) = \lVert (\alpha * \vec{s}) \odot (\vec{t} - \vec{p}) \rVert
\end{equation}
Where
\begin{itemize}
    \item $\vec{t}$ and $\vec{p}$ are the $(x, y)$ ground truth and predicted values respectively
    \item $\vec{s}$ is the original image width and height
\end{itemize}

The networks are trained for 100 epochs with $\alpha$ set to \num{2e-2}, using Nesterov momentum with a learning rate of \num{1e-1} and $l2$ regularization on the parameters with a decay of \num{1e-4}.
All of these hyper parameters were tuned using the validation set, although the possible parameter space was not fully explored due to time constraints.

\subsubsection{Evaluation}

%TODO Put in a bunch of figures and talk about this
% Problem is recreating all these experiments / networks for evaluation :/
% Might take a while...

On average, the best network achieved a 20 pixel distance error on the validation and testing sets.
While this may seem like a lot, the trailing edge extraction (and subsequent matching performance) was not severely affected when only using the start and end point predictions.

Additionally, as can be seen in Figure 3.2, the vast majority of images have a low pixel distance error, while there are a few that have higher error.
Qualitative inspection of these images shows that they are of flukes which are not the singular or major object in the image.
Ideally we could train the convolutional network to more robustly handle these cases, but since the majority of the training data have the fluke centered and focused, the network heavily biases towards handling these cases.

\subsection{Trailing Edge Scoring}

As mentioned in the beginning of this section, using only the gradient information for extracting the trailing edge works in a lot of cases, but is not a robust method.
% TODO Figures demonstrating this

If we had a score of each pixel's "trailing edginess" in an image, the trailing edge extractor could make use of this information to make better choices in trailing edge.
To do this, we need a prediction of whether or not each pixel belongs to the trailing edge of a fluke, a task that is best suited to a fully convolutional network.
In these networks, all convolutions (aside from max pooling layers) are "same" convolutions, which have square, odd filters (usually $3 \times 3$) and 1-padding.
These "same" convolutions produce an output shape that is the same as the input shape, obviating the need for any resizing.

There are three major variants on trailing edge scoring that are detailed below, however all three function on the same principle of taking an arbitrarily sized image and producing a score map of each pixel to how likely it is to be part of a trailing edge.

The dataset was sourced from existing trailing edges extracted using the gradient based algorithm, however with manual adjustments to fix many of the more common issues we encountered.
For the training set, we then extract a $128 \times 128$ patch at $128$ pixel intervals along the trailing edge (a positive patch), and a corresponding negative patch (with no trailing edge pixels) randomly sampled from the left over space above and below the trailing edge.
These are then randomly sampled into training, testing, and validation sets.

\subsubsection{Trailing Edge Scoring Architectures}

There are several network architectures that were tried, however here we will report only the major variants.
One major consideration that has to be made when selecting a fully convolutional network architecture is its receptive field.
If the receptive field is too small, it may not have enough information to accurately determine if a given pixel is part of the trailing edge.
However, in order to increase the receptive field without massively increasing the depth of a network, we must 

\paragraph{Simple}
This network is simply a stack of "same" convolutional layers, with no downsampling regions.
This has a small receptive field, but can produce detailed predictions.
Due to the small receptive field however, at convergence it gives low precision predictions, and seems to (in many cases) produce a lot of the same mistakes that normal gradient based trailing edges do.
% TODO include this architecture description in a table somewhere
% TODO include image showing trailing edge prediction from Simple network

\paragraph{Upsample}
To deal with the small receptive fields, convolutional networks typically downsample at various stages. 
This downsampling usually takes the form of max pooling (instead of e.g. convolutional kernels with a stride greater than one).
The upsample architecture is analogous to the FCN-32s architecture in \cite{long2015fully}, although we do not go down to a 1x1 spatial extent before upsampling.
This produces blocky and nearly unusable trailing edge scores, however they tend to be more connected than the disparate (but fine-grained) predictions made by the simple network.

% TODO same as simple

\paragraph{Jet}
Following the deep-jet architecture from \cite{long2015fully}, we combine softmax predictions from various stages throughout the network with predictions from "farther down" the network, cascading until we hit predictions from the last convolutional layer before any downsampling.
This gives more fine-grained predictions than the upsample network while giving somewhat better predictions than the simple network.

% TODO same as simple

\subsubsection{Using the trailing edge scores}

Once we have the map indicating 'trailing edginess', we need to combine this information with the gradient in a way that causes the trailing edge extraction algorithm to follow those pixels that the scoring map marks as trailing edge.
More formally, the trailing edge scoring map gives us an image $T_p \in [0-1]^{w \times h}$ which denotes the network's predicted probability of each pixel being part of the trailing edge.
The most simple and obvious way to combine this information with $N_y$ from before is to combine them with some weight $\beta$.

\begin{equation}
S_{te} = (1 - \beta)*N_y + \beta*(1 - T_p)
\end{equation}

We use $1 - T_p$ in this case because we are minimizing the path through $S_{te}$.
Once done, the trailing edge extraction algorithm procedes as described in the previous section.

\section{Trailing Edge Matching}

Once these trailing edges are extracted, we can use a few simple methods to compare them in search of a trailing edge that is close to the one we are finding a match for.
The simplest way to do this is to define a comparator between any two given trailing edges.
The best method for this that we have tried is to extract block integral curvature at several scales and then use Euclidean distance as a point-to-point distance measure in a dynamic time warping algorithm.
Block integral curvature approximates true integral curvature while allowing for a much faster computation time.
This approximation is made better by being taken at multiple scales.

\subsection{Curvature Measurement}

To extract the curvature, we take the trailing edge (which is a set of coordinates into an image) and a zero-image $I_0$.
Each pixel corresponding to and below (in the vertical direction) the trailing edge in $I_0$ are set to $1$.
Because the trailing edge extractor produces only one coordinate per column, we can do this safely.
Once this is done, we calculate a summed area table \cite{crow1984summed} $ST$ from $I_0$ as follows.

\begin{equation}
ST(x,y) = \sum_{i=0}^{i=y}\sum_{j=0}^{j=x} I_0 
\end{equation}

Conceptually, the next step is to slide a square of size $s$ centered on each point, and measure the percentage of that square that is within the filled in trailing edge.
To do this, we compute $BC_s(x, y)$ for each $(x, y)$ coordinate in the trailing edge.

\begin{align}
b_x &= x - \frac{s}{2}\\
b_y &= y - \frac{s}{2}\\
e_x &= x + \frac{s}{2}\\
e_y &= y + \frac{s}{2}\\
BC_s(x,y) &= \frac{(ST(b_x, b_y) + ST(e_x, e_y)) - (ST(b_x, e_y) + ST(e_x, b_y))}{s^2}
\end{align}

The set of scales to choose presents a large parameter space, however we have found that the scales $S = [5, 10, 15, 20]$ work well for our purposes.
We then treat this curvature measurement as a $\|S\| \times x$ matrix $BC$.

\subsection{Sequence Matching}

Given two sequences of curvatures $BC_1$ and $BC_2$, we match them using dynamic time warping as follows.
First, we create a cost matrix $C$ of size $x_1 \times x_2$ (where $x$ is the number of points in the original trailing edge).
We initialize this cost matrix by setting the first column and row to $\infty$, and then $C(0,0) = 0$.
Then, for each element in the cost matrix starting with $C(1,1)$, we use the following update rule

\begin{align}
D(c_1, c_2) &= \lVert c_1 - c_2 \rVert_2\\
C(i,j) &= D(x_1(i),x_2(j)) + min(C(i-1,j), C(i,j-1), C(i-1, j-1))
\end{align}

Additionally, we impose the Sakoe-Chiba \cite{sakoe1978dynamic} locality constraint $w$ so that for each element $i$ in $BC_1$, we only consider the range over elements $j$ in $BC_2$ of $j \in [min(i - w, 0), max(i + w, x_2)]$.

For most of these experiments $w$ is set to $50$, which appears to minimize the time taken for each comparison while preserving the overall accuracy of the algorithm.

\section{Alternative Approaches}

In this section, we list and briefly describe alternative approaches that were tried, although they did not prove accurate enough to make it into the final system.

\subsection{Embedding via Convolutional Networks}

\subsubsection{Raw Images}

\subsubsection{Trailing Edges}

\subsection{Aligning Trailing Edges}

\subsubsection{Keypoint Alignment}

\subsubsection{Dynamic Time Warping Alignment}

\subsection{Histogram Matching}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
