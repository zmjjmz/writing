%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER THREE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
\chapter{Methods} \label{sec:methods}

In this chapter, we detail the finalized algorithm pipeline, as well as some alternative approaches that we found to have limited successs.

\section{Trailing Edge Extraction}

Extracting good, high quality trailing edges images is one of the primary challenges when matching Humpback whales by their trailing edge.
In this section, we detail the steps that go into automating the extraction of high quality trailing edges, while trying to minimize human intervention.

One major assumption that we make when extracting these trailing edges is that the Humpback whale fluke is aligned such that its major axis is horizontal.
Additionally, we generally assume that all parts of the fluke are present, however theoretically it would be possible to match a partial fluke -- although unlikely.

While these assumptions do not make for an incredibly robust system, the nature of the problem (and the dataset that we had at hand) makes these assumptions reasonable.

\subsection{Basic Contour Extraction Algorithm}

The base algorithm that is used for extracting the trailing edge uses the vertical gradient information of the image (denoted as $I_y$).
We extract $I_y$ using a vertically oriented $5 \times 5$ Sobel kernel\cite{Sobel1968}.  

We then normalize $I_y$ with min-max scaling, giving $N_y$ as

\begin{equation} \label{norm01}
N_{y} = \frac{I_y - min(I_y)}{max(I_y) - min(I_y)}
\end{equation}

With $N_y$, we then need a starting point and ending point for the algorithm, denoted $s$ and $e$ respectively. 
For our purposes, we use the left and right tips of the fluke as our start and end.
We explain how these points are determined in a later section.
We then take the columns corresponding to these points and set every pixel in $N_y(s_x,\cdot)$ (i.e. the column $s_x$) and $N_y(e_x,\cdot)$ to $\infty$,  and then set those points in $N_y$ to 0. 
This forces the path to start and end at these points (as we are finding the minimal path between them).

The minimal path is then found by scanning the columns of $N_y$ from left to right, starting and ending at $s$ and $e$ respectively. 
For each pixel in a column we set its cost with the update rule

\begin{equation} \label{te_update}
C(x,y) = min_{y_c=(y-n)}^{y+n}(C(x-1, y_c) + N_y(x,y))
\end{equation}

Where $n$ is a neighborhood constraint which we default to $2$, meaning that each pixel considers $5$ 'neighbors' in the previous column.

If $y_c$ is out of bounds, we let $C(x-1, y_c) = \infty$, and if $x - 1 < 0$, we let $C(x-1, \cdot) = 0$.

As $C$ is filled out, we also keep a backtrace matrix $B$, which keeps track of the index of the minimal candidate chosen in equation 3.2.
Once the end column is reached, we work backwards from $e$ to construct the path, adding the coordinate corresponding to $B(x,y)$ at each step.

% TODO: Put in actual algorithm pseudocode

We can also extend this algorithm by adding extra 'control points' which the path is forced through, using the same methodology as forcing the path through the start and end points.
Commonly, we use the bottom of the notch as a control point, although this affects accuracy negatively if it is inaccurate.

While this algorithm has no understanding of Humpback whale flukes, a lot of images that are constrained around the Fluke with oceanic backgrounds (which is a large majority of the dataset at hand) provide high quality trailing edges when put through this algorithm.
However, this is (obviously) not a robust algorithm for finding trailing edges, something that we will attempt to fix later on.

\subsection{Fluke keypoint prediction}

One major issue with automating the above trailing edge extraction algorithm is that it requires manual annotation in the selection of the starting and ending points (i.e., the fluke keypoints), as well as any control points (specifically the bottom of the notch).
To work around this, we propose a convolutional network that predicts these tip points as part of the identification pipeline.

The convolutional network does not need full-sized images, so the first step of the keypoint extraction pipeline is to resize the image to $256 \times 256$ pixels.
This size choice is somewhat arbitrary, but we find that it provides the best performance without using an unnecessary amount of memory.
The network then predicts three points (left tip, right tip, and the bottom of the notch) in the range $[0-1]$ for both $x$ and $y$. 
This is then rescaled back up to the original image size.
%The evaluated network architectures are detailed in Table 3.1. % TODO put this in

\subsubsection{Network Design}

The overall design of the network follows the pattern of alternating small ($3 \times 3$) convolutional filters with $2 \times 2$ max pooling layers, at each step doubling the number of channels (starting with $8$ channels).
This is somewhat similar to VGG-16, although with half the trainable layers.
After a $32\times$ downsample has been achieved, we attach a decision layer which consists of a dense layer followed by three separate dense layers with separate predictions layers after (one for each point being predicted).
While this is not a common approach in keypoint prediction, we found that it gave better performance than having the points predicted as a single vector.

We theorize that this may be because shared units between each of the three predictions leads to stronger correlations between them, reducing overall prediction flexibility.

\subsubsection{Training Details}

Generating the training data for this is straightforward given a set of annotations with the associated points to learn.
% TODO: Once we've cleared up what the datasets are, put in the details of how the training / val / test sets were made
The dataset that we created for this purpose contains approximately $2700$ training images, $900$ validation images, and $1200$ test images.
Despite the small size of the training set, we found that it generalizes pretty well.

First, each image is resized to a fixed width while maintaining the aspect ratio.
This is done to somewhat normalize the relative scale of objects in each image on the assumption that they are constrained to contain the fluke.
Each image is rescaled to the network size, and then the corresponding targets are rescaled to the range $[0-1]$.
The size of the original image is recorded as well.
While it would be possible to treat this as a simple multi-variate regression and use RMSE loss, we achieved better results by averaging the Euclidean distance between the predicted points and true points.
We also include a scalar scaling factor $\alpha$, which scales each point by a proportion of the original image size.
Thus, we have the scaled Euclidean loss $SE$

\begin{equation}
SE(\vec{t}, \vec{p}, \vec{s}, \alpha) = \lVert (\alpha * \vec{s}) \odot (\vec{t} - \vec{p}) \rVert
\end{equation}
Where
\begin{itemize}
    \item $\vec{t}$ and $\vec{p}$ are the $(x, y)$ ground truth and predicted values respectively
    \item $\vec{s}$ is the original image width and height
\end{itemize}

The networks are trained for 100 epochs with $\alpha$ set to \num{2e-2}, the Adam optimizer\cite{kingma2014adam} (with the recommended settings) and $l2$ regularization on the trainable parameters with a decay of \num{1e-4}.
All of these hyper parameters were tuned using the validation set, although the possible parameter space was not fully explored due to time constraints.

\subsubsection{Evaluation} % TODO: Maybe this should be moved to the results section?

%TODO Put in a bunch of figures and talk about this
% Problem is recreating all these experiments / networks for evaluation :/
% Might take a while...

On average, the best network achieved a 10 pixel distance error on the validation and testing sets (in the original image scale).
While this may seem like a lot, the trailing edge extraction (and subsequent matching performance) was not severely affected when only using the start and end point predictions.

%Additionally, as can be seen in Figure 3.2, 
We find that, for the vast majority of images, the network achieves a low pixel distance error, while there are a few that have a much higher error.
Qualitative inspection of these images shows that they are of flukes which are not the singular or major object in the image, nor horizontally oriented.

We attempted to use a spatial transformer network\cite{jaderberg2015spatial} to try and handle these cases, but we were unable to get it to perform as well as the standard convolutional network, nor produce sensible transformations.

\subsection{Trailing Edge Scoring}

As mentioned in the beginning of this section, using only the gradient information for extracting the trailing edge works in a lot of cases, but is not a robust method.
% TODO Figures demonstrating this

If we had a score of each pixel's "trailing edginess" in an image, the trailing edge extractor could make use of this information to make better choices in trailing edge extraction.
To do this, we need a prediction of whether or not each pixel belongs to the trailing edge of a fluke, a task that is best suited to a fully convolutional network.
In these networks, all convolutions (aside from max pooling layers) are "same" convolutions, which have square, odd filters (usually $3 \times 3$) and 1-padding.
These "same" convolutions produce a spatial output shape that is the same as the input shape, obviating the need for any interpolation.

The four major variants on trailing edge scoring networks that we evaluated are detailed below.
All of these networks function on the same paradigm of taking an arbitrarily sized image and producing an image of the same size but with a class score for each pixel.

The dataset was sourced from existing trailing edges extracted using the gradient based algorithm, however with manual adjustments to fix many of the more common issues we encountered with this algorithm.

To generate the training set, we then extract a $128 \times 128$ patch at $128$ pixel intervals along the trailing edge (a positive patch), and a corresponding negative patch (with no trailing edge pixels) randomly sampled from the left over space above and below the trailing edge.
These patches are then randomly split into training, testing, and validation sets each with $3700$, $1200$, and $1600$ patches respectively.

\subsubsection{Trailing Edge Scoring Architectures}

There are several network architectures that were tried, however here we will report only the major variants.
One major consideration that has to be made when selecting a fully convolutional network architecture is its receptive field.
If the receptive field is too small, it may not have enough information to accurately determine if a given pixel is part of the trailing edge.
However, in order to increase the receptive field without massively increasing the depth of a network, we must 

\paragraph{Simple}
This network is simply a stack of $6$ "same" convolutional layers (of decreasing spatial extent), with no downsampling regions.
This has a small receptive field, but can produce detailed predictions.
Due to the small receptive field however, at convergence it gives low precision predictions, and seems to (in many cases) produce a lot of the same mistakes that normal gradient based trailing edges do.
% TODO include this architecture description in a table somewhere
% TODO include image showing trailing edge prediction from Simple network

\paragraph{Upsample}
To deal with the small receptive fields, convolutional networks typically downsample at various stages. 
This downsampling usually takes the form of max pooling (instead of e.g. convolutional kernels with a stride greater than one).
The upsample architecture is analogous to the FCN-32s architecture in \cite{long2015fully}, although we do not go down to a 1x1 spatial extent before upsampling.
This produces blocky and nearly unusable trailing edge scores, however they tend to be more connected than the disparate (but fine-grained) predictions made by the simple network.

% TODO same as simple

\paragraph{Jet}
Following the deep-jet architecture from \cite{long2015fully}, we combine softmax predictions from various stages throughout the network with predictions from "farther down" the network, cascading until we hit predictions from the last convolutional layer before any downsampling.
This gives more fine-grained predictions than the upsample network while giving somewhat better predictions than the simple network.

% TODO same as simple

\paragraph{Residual}
While the receptive field of the Simple network is small, it can produce very fine trailing edges, which we found to be necessary for good trailing edge extraction.
However with such a small receptive field it can be more prone to making mistakes.
It is possible to create a very deep network of 'same' convolutions, however training very deep networks like this can run into problems very quickly.
Recently, there has been work on making very deep networks feasible by adding residual connections\cite{he2015deep}, showing that these very deep networks can achieve high accuracy in a challenging benchmark.
We replicate this architecture by stacking $64$ $3\times3$ 'same' convolution kernels, and adding a residual connection every other layer.
This performs better than the Simple network, however we do not see a marked improvement in trailing edge matching as a result.

\subsubsection{Using the trailing edge scores}

Once we have 'trailing edginess' score for each pixel, we need to combine this information with the gradient in a way that causes the trailing edge extraction algorithm to follow those pixels that the scoring map marks as trailing edge.
More formally, the trailing edge scoring map gives us an image $T_p \in [0-1]^{w \times h}$ which denotes the network's predicted probability of each pixel being part of the trailing edge.
The most simple and obvious way to combine this information with $N_y$ from before is to combine them with a mixing parameter $\beta$.

\begin{equation}
S_{te} = (1 - \beta)*N_y + \beta*(1 - T_p)
\end{equation}

We use $1 - T_p$ in this case because we are minimizing the path through $S_{te}$.
Once done, the trailing edge extraction algorithm procedes as described in the previous section.

For most of the evaluation process, we simply set $\beta = 0.5$.

Another variant on combining $T_p$ and $N_y$ that we tried was to dilate the trailing edge predictions and then forbid the trailing edge from going outside of those predictions.
However one of the main issues with this is that if there are any breaks in the prediction (which can happen even with the Upsample architecture), the trailing edge cannot be extracted, leading to broken trailing edges.

\subsubsection{Training Details}

All networks were trained for 100 epochs (or until convergence) with a batch size of $32$, with $l2$ regularization using a decay of \num{1e-4}.
We used the Adam optimizer\cite{kingma2014adam} (with the recommended settings) for calculating weight updates.

One detail that turned out to be important is the class imbalance.
The trailing edge pixels (necessarily) make up a small percentage of the total image, meaning that these networks could get a fairly high accuracy (and thus low loss) simply by predicting only background pixels.
In order to prevent this, we only sample a negative patch once for every positive patch (as detailed above), and additionally we weight the loss for the trailing edge pixels $10\times$ higher than the loss for the background pixels.

Due to the nature of the training data, we were concerned that the networks would simply learn to replicate the function that generated the trailing edges, despite human corrections.
In order to mitigate this issue, we included some non-spatial data augmentation, namely random Gaussian blur and randomly inverting the pixel intensities.
Unfortunately this had limited success. % TODO: Figure showing that the trailing edge finder kinda still works on gradient

All of these hyper parameters were tuned using the validation set, although the possible parameter space was not fully explored due to time constraints.

\subsubsection{Evaluation}

% TODO: Figure out accuracies, and if the below paragraph still holds.

Due to the overwhelming class imbalance, the model accuracy is rather meaningless (e.g., the accuracy of an all-background prediction is $99\%$).
Instead, we report the intersection-over-union (IoU) score, which gives a much better idea of model performance.
We also report the precision and recall of the model.

% IOU / PRECISION / RECALL
% TRAIN / TEST / VAL
% SIMPLE / UPSAMPLE / JET / RES


\begin{table*}[!htb]%
	\centering
	\resizebox{\linewidth}{!}
	{
		\begin{tabular} {l || l | l | l || l | l | l || l | l | l |}
		& \multicolumn{3}{c||}{Training} & \multicolumn{3}{c||}{Validation} & \multicolumn{3}{c|}{Testing} \\
		\hline
		Architecture & Pr. & Re. & IoU & Pr. & Re. & IoU & Pr. & Re. & IoU \\
		\hhline{=#===#===#===|}
		Simple & 0.59 & 0.95 & 0.57 & 0.59 & 0.94 & 0.57 & 0.60 & 0.95 & 0.59 \\
		\hline
		Upsample & 0.17 & 0.88 & 0.17 & 0.17 & 0.86 & 0.17 & 0.17 & 0.87 & 0.17 \\
		\hline
		Jet & 0.62 & 0.89 & 0.57 & 0.62 & 0.88 & 0.57 & 0.63 & 0.89 & 0.58 \\
		\hline
		Residual & 0.57 & 0.93 & 0.54 & 0.57 & 0.92 & 0.54 & 0.58 & 0.93 & 0.56 \\
		\hline
		\end{tabular}
	}
	\caption{Table showing the precision, recall, and IoU of each of the evaluated trailing edge scorers on each section of the trailing edge dataset. For the purposes of this analysis, we use the \texttt{argmax} over the classes to determine a positive (i.e. trailing edge) or negative pixel.}
	\label{tab:te_score_full_analysis}
\end{table*}

Initially, we thought that precision would be the most important metric for evaluating networks (which is reflected in the class weighting).
However, we found that even a low precision classifier could give good trailing edges, and that the important measure was how detailed these trailing edges were.


This is likely because small 'blips' of trailing edge would not be chosen by the trailing edge extractor.

\section{Trailing Edge Matching}

Given extracted trailing edges, we define a method for doing a one-to-one comparison between a given query and database trailing edge.
%Once these trailing edges are extracted, we can use a few simple methods to compare them in search of a trailing edge that is close to the one we are finding a match for.
The simplest way to do this is to define a (potentially non-metric) distance function between any two trailing edges.
Once this distance function is defined, we can identify an individual by its trailing edge (referred to as the query trailing edge) by looking at the identity of the closest trailing edge in the database.
As a distance function we propos dynamic time warping over the block curvature measurements, using a weighted Euclidean distance as a local distance function between curvatures.

\subsection{Curvature Measurement}

In order to do this, we must first extract the curvature from the trailing edge.
Given the trailing edge as a sequence of coordinates into the original image, we construct a zero-image $I_0$ of shape similar to the original image.
Each pixel corresponding to and below the trailing edge in $I_0$ is set to $1$.
Because the trailing edge extractor produces only one coordinate per column, we can do this safely, however this algorithm could be easily adapted to this not being the case.
Once this is done, we calculate a summed area table \cite{crow1984summed} $ST$ from $I_0$ as follows.

\begin{equation}
ST(x,y) = \sum_{i=0}^{i=y}\sum_{j=0}^{j=x} I_0 
\end{equation}

Conceptually, the next step is to slide a square of shape $s \times s$ centered on each point, and measure the percentage of that square that is within the filled in trailing edge.
These values $s$ are the different scales at which we meassure curvature, and are computed as a percentage of the trailing edge width.

To do this, we compute $BC_s(x, y)$ for each $(x, y)$ coordinate in the trailing edge.

\begin{align}
b(i) &= i - \frac{s}{2}\\
e(i) &= i + \frac{s}{2}\\
BC_s(x,y) &= \frac{(ST(b(x), b(y)) + ST(e(x), e(y))) - (ST(b(x), e(y)) + ST(e(x), b(y)))}{s^2}
\end{align}

The numerator in 3.8 gives the total area within the square that is below the trailing edge, which we then normalize by dividing by the square's area.

The set of scales to choose presents a large parameter space, however we have found that the scales $S = [1\%, 2\%, 3\%, 4\%]$ work well for our purposes.
We then treat this curvature measurement as a $\|S\| \times w$ matrix $BC$, where $w$ is the length of the trailing edge.

\subsection{Sequence Matching}

Given two sequences of curvatures $BC_1$ and $BC_2$ (referred to as query and database curvature respectively) we match them using dynamic time warping as follows.
First, we create a cost matrix $C$ of size $w_1 \times w_2$. 
We initialize this cost matrix by setting the first column and row to $\infty$, and then $C(0,0) = 0$, intuitively forcing the optimal path to match align the beginning of $BC_1$ with $BC_2$.
Then, for each cell $(i,j)$ in the cost matrix starting with $C(1,1)$, we use the following update rule

\begin{align}
D_{s_w}(c_1, c_2) &= \lVert s_w \odot (\vec{c_1} - \vec{c_2}) \rVert_2\\
C(i,j) &= D_{s_w}(BC_1(i,\cdot),BC_2(j,\cdot)) + min(C(i-1,j), C(i,j-1), C(i-1, j-1))
\end{align}

Where $s_w$ is a vector giving the weight for each curvature scale, and $\vec{c}$ is a vector of the curvatures at different scales for a point.

Additionally, we impose the Sakoe-Chiba \cite{sakoe1978dynamic} locality constraint $T$ so that for each element $i$ in $BC_1$, we only consider the range over elements $j$ in $BC_2$ of $j \in [min(i - T, 0), max(i + T, w_2)]$.

We set $T$ as a percentage of $w_1$, which is the "query" trailing edge.
For most of these experiments $T$ is set to $10\%$, which appears to minimize the time taken for each comparison while preserving the overall accuracy of the algorithm.

It's worth noting that while this distance measure is not a metric distance (i.e. it doesn't satisfy the triangle inequality), it is a symmetric distance as $D_{s_w}(\cdot,\cdot)$ is symmetric\cite{muller2007information}.

\section{Alternative Approaches}

In this section, we list and briefly describe alternative approaches that were tried, although they did not prove accurate enough to make it into the final system.

\subsection{Aligning Trailing Edges}

One obvious pre-processing step that would make sense when comparing trailing edges is to make sure that they are aligned in image space.
However, we found that doing so when comparing curvature was often unnecessary (due to the invariances to rotation and translation, scale was taken care of separately), and using the Euclidean distance between points on the aligned trailing edges (i.e. as $D(\cdot,\cdot)$) did not give good results.

There were two approaches to alignment that we evaluated, although neither achieved top-1 accuracies above $20\%$.

\subsubsection{Keypoint Alignment}

As noted in the section on fluke keypoints, there are three points to predict -- left, notch and right.
Originally the intention for recording (and predicting) all three, as opposed to just left and right, was to have three corresponding points with which to estimate an affine transformation from database image onto query image.
This would be done prior to any computation of trailing edge or curvature.

One major issue with aligning these iamges however is that if a non-affine transformation was required, the trailing edge itself would be warped in such a way that made matching difficult.

\subsubsection{Dynamic Time Warping Alignment}

Taking after the AI-DTW approach laid out in \cite{qiao2006affine}, we ran an iterative alignment process that used the correspondences found by DTW (using either curvature distance or Euclidean distance as criteria).
Essentially this method would find alignments using DTW, and then use these alignments to estimate an affine transformation of the database image onto the query image -- and then repeat until convergence.

However, we found that this process oftentimes wouldn't converge, and when it did the alignments provided were of worse quality than those found by aligning the three fluke keypoints.
Additionally, the extra time taken to carry out this process was impractical.

\subsection{Histogram Matching}

One early curvature comparison method that we evaluated was to use histograms to match instead of a sequence-based method.
This is a common approach for comparing curvatures \cite{kumar2012leafsnap}, however we found that for our purposes even high resolution histograms did not provide enough detail to match the trailing edges properly, although this could potentially be explored further.

\subsection{Embedding via Convolutional Networks}

We also made an attempt at training convolutional networks to directly embed the images of flukes into a $n$ dimensional vector, much like \cite{schroff2015facenet} and \cite{parkhi2015deep}.
However, most of the previous literature on this technique is applied to larger datasets such as LFW\cite{huang2007labeled}, which is significantly larger than the dataset that we had available.
A major factor in this is that these larger datasets often have five to ten images per identity (if not more), whereas most of the identities in our dataset had one or two images associated.

Regardless, we attempted the embedding approach (from raw images), however even a severely overfit convolutional network only achieved half the top-1 accuracy on its training set that the main method is able to achieve.
We tried both triplet loss (a modified version of the one detailed in \cite{schroff2015facenet}) and contrastive loss \cite{hadsell2006dimensionality} to no avail.

We believe that the small amount of images per identity is the main factor for the failure of these methods, and that a larger dataset would be necessary to properly train them.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
