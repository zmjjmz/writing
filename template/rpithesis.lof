\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf {Example Flukes}. Example images of humpback whale flukes from the SPLASH \cite {calambokidis2008splash} dataset. These flukes both have distinctive internal textures (more so on the left). However, the trailing edge on the left is far more distinctive than the trailing edge on the right.\relax }}{2}
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Uniform Internal Texture}. This image of a humpback fluke shows no clear internal texture, but a distinctive trailing edge.\relax }}{3}
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {Change in Trailing Edge}. The above images show that out of plane rotations of the fluke can obscure it or otherwise make it hard to match. These images are both of the same individual, however in the top image the fluke is rotated slightly towards the camera.\relax }}{4}
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf {Example Keypoint Prediction}. Example image showing the left tip, bottom of the notch, and right tip located by the keypoint extractor convolutional network.\relax }}{12}
\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf {Example Keypoint Failure}. Example image showing a keypoint extraction failure case from its testing set. Note the difference in pose of the fluke from the success case shown in Figure 3.1\hbox {}. This is an example of a fluke image that violates our assumptions.\relax }}{14}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf {Histogram of Keypoint Distances}. This is a histogram of the average distance from predicted keypoints to annotated keypoints on the testing set for the keypoint extraction network. The vast majority of keypoints are predicted within 10 pixels of the true keypoints.\relax }}{15}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf {Example Trailing Edge Extraction}. Example of the baseline trailing edge extraction with $n = 2$. Note that the gradient image has a significant black area where the trailing edge is, making this an easy case.\relax }}{16}
\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf {Example Trailing Edge Score}. Bottom image is the Residual scorer's classification of the top image. Trailing edge is class is colored black.\relax }}{19}
\contentsline {figure}{\numberline {3.6}{\ignorespaces \textbf {Trailing Edge Scores}. These are the trailing edge scores given by each of the networks described in section 3.1.3.1\hbox {} on the image used in Figure 3.5\hbox {}.\relax }}{21}
\contentsline {figure}{\numberline {3.7}{\ignorespaces \textbf {Trailing Edge Curvature}. The top images are of the same individual, and the bottom images visualize the corresponding curvatures for the trailing edges that were extracted. Each row in the visualization is a curvature scale, increasing from top to bottom. Note that darker blue implies a ``valley'' in the trailing edge, whereas lighter blue implies a ``peak''.\relax }}{25}
\contentsline {figure}{\numberline {3.8}{\ignorespaces \textbf {Example Matches}. The left side shows a success case, and the right side shows a failure case.\relax }}{27}
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Varying Manual Extraction}. We can see here that the difference in using the manually annotated points (purple) provided for this dataset versus the keypoint extractor's predicted points (cyan) produces a small drop in top-1 matching accuracy.\relax }}{31}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {Varying Keypoint Image Size}. While the $128 \times 128$ size network is better than the $256 \times 256$ network (green and blue respectively), we believe the difference between them to be insignificant. The $64 \times 64$ network (red) is clearly inferior however. \relax }}{32}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Varying Crop Strategy}. Cropping images around the trailing edge and then resizing them proves to be very important, not doing so gives a very low accuracy. We can see in Figure \G@refundefinedtrue \text {\normalfont \bfseries ??}\GenericWarning { }{LaTeX Warning: Reference `fig:chip_size_hist' on page 34 undefined} that there is a wide distribution of image sizes, which can hamper the effectiveness of DTW.\relax }}{34}
\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf {Distribution of Image Widths}. The image width distribution (left) is fairly wide, with most of the mass centered between 600 and 800.\relax }}{35}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces Distribution of Image Widths}}{35}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces Distribution of Trailing Edge Lengths}}{35}
\contentsline {figure}{\numberline {4.5}{\ignorespaces \textbf {Varying Crop Size}. Since we crop around the start and end points of the trailing edge, this crop size effectively controls the length of the extracted trailing edge. Note that we use the manually annotated points in this analysis to control for any issues with keypoint extraction.\relax }}{35}
\contentsline {figure}{\numberline {4.6}{\ignorespaces \textbf {Trailing Edge Scorer Architectures}. The highest performing trailing edge scorer (Residual) is shown in red, followed by Simple, Jet, and Upsample (in descending order of accuracy).\relax }}{36}
\contentsline {figure}{\numberline {4.7}{\ignorespaces \textbf {Varying $\beta $}. It's clear from this that the optimal $\beta $ is $0.5$, although it is interesting to note that using only the network's trailing edge scores provides better accuracy than not using it at all.\relax }}{37}
\contentsline {figure}{\numberline {4.8}{\ignorespaces \textbf {Varying $n$}. This shows that the optimal neighborhood constraint is $n = 1$, despite qualitatively producing worse-looking trailing edges.\relax }}{38}
\contentsline {figure}{\numberline {4.9}{\ignorespaces \textbf {Curvature Diversity}. Left panel (a) shows the average standard deviation of the (fixed length) curvature at different scales. Right panel (b) shows the average Euclidean distance between successive scales of curvature\relax }}{39}
\contentsline {figure}{\numberline {4.10}{\ignorespaces \textbf {Varying Curvature Scales}. The scales evaluated here are parameterized with a start, end, and step size. We always start at 2\%, and vary the step size between 1 \& 2 and the end between 8\% \& 10\%.\relax }}{40}
\contentsline {figure}{\numberline {4.11}{\ignorespaces \textbf {Varying $s_w$}. The yellow bar shows $W = 1$, i.e.\ all curvatures weighted equally.\relax }}{41}
\contentsline {figure}{\numberline {4.12}{\ignorespaces \textbf {Varying Sakoe-Chiba bound}\relax }}{42}
\contentsline {figure}{\numberline {4.13}{\ignorespaces \textbf {Varying Decision Criterion}\relax }}{42}
