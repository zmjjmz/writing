\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf {Example Flukes}. Example images of humpback whale flukes from the SPLASH \cite {calambokidis2008splash} dataset. These flukes both have distinctive internal textures (more so on the left). However, the trailing edge on the left is far more distinctive than the trailing edge on the right.\relax }}{2}
\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf {Uniform Internal Texture}. This image of a humpback fluke shows no clear internal texture, but a distinctive trailing edge.\relax }}{3}
\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf {Change in Trailing Edge}. The above images show that out of plane rotations of the fluke can obscure it or otherwise make it hard to match. These images are both of the same individual, however in the top image the fluke is rotated slightly towards the camera.\relax }}{4}
\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf {Example Keypoint and Trailing Edge Annotation}. This image shows a typical set of fluke keypoints and trailing edge extracted by our algorithm.\relax }}{6}
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf {Example Keypoint Prediction}. Example image showing the left tip, bottom of the notch, and right tip located by the keypoint extractor convolutional network.\relax }}{13}
\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf {Example Keypoint Failure}. Example image showing a keypoint extraction failure case from its testing set. Note the difference in pose of the fluke from the success case shown in Figure 3.1\hbox {}. This is an example of a fluke image that violates our assumptions.\relax }}{15}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf {Histogram of Keypoint Distances}. This is a histogram of the average distance from predicted keypoints to annotated keypoints on the testing set for the keypoint extraction network. The vast majority of keypoints are predicted within 10 pixels of the true keypoints.\relax }}{16}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf {Example Trailing Edge Extraction}. Example of the baseline trailing edge extraction with $n = 2$. Note that the gradient image has a significant black area where the trailing edge is, making this an easy case.\relax }}{18}
\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf {Example Trailing Edge Score}. Bottom image is the Residual scorer's classification of the top image. Trailing edge is class is colored black.\relax }}{20}
\contentsline {figure}{\numberline {3.6}{\ignorespaces \textbf {Trailing Edge Scores}. These are the trailing edge scores given by each of the networks described in section 3.1.3.1\hbox {} on the image used in Figure 3.5\hbox {}. A darker pixel is predicted to be part of the trailing edge by the network.\relax }}{22}
\contentsline {figure}{\numberline {3.7}{\ignorespaces \textbf {Trailing Edge Scoring Failure}. Unfortunately even the Residual architecture is still imperfect, and can lead to catastrophic trailing edge failures like this one. However this is a rare case.\relax }}{24}
\contentsline {figure}{\numberline {3.8}{\ignorespaces \textbf {Trailing Edge Curvature}. The top images are of the same individual, and the bottom images visualize the corresponding curvatures for the trailing edges that were extracted. Each row in the visualization is a curvature scale, increasing from top to bottom. Note that darker blue implies a ``valley'' in the trailing edge, whereas lighter blue implies a ``peak''.\relax }}{27}
\contentsline {figure}{\numberline {3.9}{\ignorespaces \textbf {Example Matches}. The left side shows a success case, and the right side shows a failure case.\relax }}{29}
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Score Separability Histogram}. The blue bars in this figure represent true matches, and the red bars represent false matches. The line at $\text {score} = 0.88$ represents the optimal threshold at which to accept a match, although we can see it is not perfect.\relax }}{35}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {Varying Manual Extraction}. There is a small difference in matching accuracy between using the manually annotated points (red) provided for this dataset versus the keypoint extractor's predicted points (cyan). The bottom of the notch keypoint is not used in these evaluations.\relax }}{36}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Distribution of Unresized Trailing Edge Lengths}. This shows a significant distribution of trailing edges centered around a width of 800 pixels.\relax }}{37}
\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf {Varying $w$}. Note that we use the manually annotated points in this analysis to control for any issues with keypoint extraction.\relax }}{38}
\contentsline {figure}{\numberline {4.5}{\ignorespaces \textbf {Trailing Edge Scorer Architectures}. The highest performing trailing edge scorer (Residual) is shown in red, followed by Simple, Jet, and Upsample (in descending order of accuracy).\relax }}{39}
\contentsline {figure}{\numberline {4.6}{\ignorespaces \textbf {Trailing Edge Scorer Architectures at $\beta = 1$}. Upsample (cyan) performs significantly worse than the other networks, which all perform comparably.\relax }}{39}
\contentsline {figure}{\numberline {4.7}{\ignorespaces \textbf {Varying $\beta $}. Setting $\beta = 0.5$ (yellow) provides only marginally better results over any other non-zero value of $\beta $, but is significantly better than $\beta = 0$ (purple).\relax }}{40}
\contentsline {figure}{\numberline {4.8}{\ignorespaces \textbf {Example Use of Trailing Edge Scorer}. In (a), we have the trailing edge extracted with the Residual scorer. Compare to (b), which did not use any scorer at all, resulting in a match failure.\relax }}{41}
\contentsline {figure}{\numberline {4.9}{\ignorespaces \textbf {Varying $n$}. This shows that the optimal neighborhood constraint is $n = 3$ (purple), despite qualitatively producing worse-looking trailing edges. After $n = 5$ (blue), the trailing edges can become very noisy affecting match accuracy.\relax }}{42}
\contentsline {figure}{\numberline {4.10}{\ignorespaces \textbf {Curvature Diversity}. Left panel (a) shows the average standard deviation of the (fixed length) curvature at different scales. Right panel (b) shows the average Euclidean distance between curvatures measured at successive scales.\relax }}{43}
\contentsline {figure}{\numberline {4.11}{\ignorespaces \textbf {Varying Sakoe-Chiba Bound}. We achieve good results with the Sakoe-Chiba bound set to $10\%$ (yellow), although we can get slightly better results with it set to $50\%$ (green) at the expense of computation time.\relax }}{44}
\contentsline {figure}{\numberline {4.12}{\ignorespaces \textbf {Example Disagreements Between Hotspotter and our method}. On the left side, (a) was matched correctly to (c) by our method, whereas Hotspotter could not find any matches for (a). On the right hand side however, Hotspotter gave (d) as the top match for (b) despite a large variance in pose and lighting, while our method failed to rank (d) in the top 5 matches for (b).\relax }}{44}
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Histogram of Ground-Truth Ranks}. Note that the histogram ranges are uneven to better show the lower end of the range. In order to have all matches found within the top-$k$ matches we would have to set $k = 414$.\relax }}{48}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Histogram of Score Differences between 1st and 2nd Rank}. Note that the histogram ranges are uneven to better show the higher end of the range.\relax }}{49}
