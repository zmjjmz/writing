
After the GZGC concluded, we reprocessed and reanalyzed all images contributed by all citizen scientists; for consistency, and to ensure accuracy, we reprocessed the images submitted for feedback generation.   All images across all client machines were accumulated and given to the a server for processing.  The accumulated analysis with IBEIS followed the following procedure:

%The complete results of a participant's $N_i$ processed images could be made available by publicly hosting it or otherwise delivered for viewing, although this was not done for the GZGC. For a publicly-visible viewing, the identification code on the registration card would allow participants to not only locate their complete results but will also provide some level of anonymity to other participants and to the administrators of the event for blind analysis

\begin{enumerate}
	\item IBEIS semi-automatically found detection regions (i.e.\ bounding boxes around potential sightings) of zebras (and giraffes) in all of the images contributed by all citizen scientists,
	\item A reviewer manually checked the detection regions by altering poor detections, removing false-positive detections, and adding false-negative detections,
	\item A reviewer manually added relevant metadata to the detections, specifically: viewpoint of the animal and quality of the sighting for that animal,
	\item A filtering step eliminated detections that violated the collection protocol (non-left viewpoint images) or were considered of insufficient quality,
	\item IBEIS incrementally processed each surviving sighting to find matching animals from the recognition database,
	\item A reviewer used the results of the appearance-based matching and the IBEIS interface to either assign a new name to the unknown individual or mark it as a repeated sighting,
	\item A reviewer identified \textit{split} and \textit{join} name scenarios, described in Section \ref{sec:identification}, where an error occurred during the name assignment, and finally,
	\item After naming was completed, a final review step allowed a trained biologist to age and sex the individual using all of its sightings.
\end{enumerate}

For each of the review steps, a review interface was added to IBEIS to help streamline and parallelize the work of manually reviewing  (or \textit{turking}) the results.  These \textit{turking interfaces} have been fully integrated into IBEIS as a web interface and are shown in Figures \ref{fig:turking_interface_detection}, \ref{fig:turking_interface_viewpoint}, \ref{fig:turking_interface_quality}, and \ref{fig:turking_interface_metadata}.  It should be noted that the IBEIS software processed the sightings incrementally, in chronological order, such that the assigned name identifiers of newly sighted animals followed the order of when they were sighted during the GZGC.

All of the turking interfaces were designed with high parallelization and accuracy in mind.  Because of these attributes, the turking interfaces were implemented as a web portal that would allow a reviewer to access a series of unreviewed detection regions or to assign viewpoint, age, or sex metadata to an already reviewed detection region.  To accomplish the parallelization, the interfaces employed randomization to prevent two or more reviewers from working on the same image.  Every time the web turking web page was refreshed in the internet browser, a new, random, and unreviewed image was given to the reviewer.

It should be noted that no locking, checkout, or reservation system was employed to implement the parallelized system.  The lack of a reservation system did introduce some duplicated turking effort as the number of unreviewed images began to decease (to around twice the number of active reviewers) because two reviewers could have been given the same random image to annotate.  However, this cost was considered worth paying as it significantly reduced the complexity of the overall turking system.  In practice, this scheme worked well for our turking problem because the time it took for a reviewer to review an image was generally very short, from less than 1 second to 10 seconds.  In the event an image was randomly selected more than once (and given to two different reviewers) before being marked as reviewed, the last submitted reviewer's decision recorded over all previous reviewer's decision.  This inefficiency was the inherent trade-off paid to forgo a much more sophisticated reservation system.

As an optional feature, the turking interfaces allowed for an administrator mode that provided a verification procedure during annotation.  When an image was submitted by a reviewer, a random selection (10 percent) was mirrored to an administrator's monitor for review.  If the administrator agreed with the reviewer's decision, nothing more needed to be done and no action needed to be completed; conversely, if the administrator disagreed, the event gave the administrator and reviewer a chance to reconcile the inconsistency and administer further training to adhere with the turking protocol.  The turking system kept a running average of each reviewer's success / failure rate.  When that ratio fell below a specified threshold for a particular reviewer, that individual was suggested to take a break or undergo an additional session of training.

\begin{figure}[t]%
%\begin{figure}[!htb]%
	\centering
	\subfloat {{ $\vcenter{\hbox{ \includegraphics[width=0.75\textwidth]{resources/turk_detection.png} }}$ }}%
    	\caption[The Detection Region Turking Web Interface]{\textbf{The detection region turking web interface.}  The web interface allowed multiple reviewes to add or remove detection regions (bounding boxes) to an image through a distributed, highly-parallelized annotation system.}
    	\label{fig:turking_interface_detection}
\end{figure}

\section{Detection}
After the automatic image detection processing was completed by IBEIS, a reviewer had to manually check for poor detections (bounding boxes that did not properly constrain the individual), false detections that weren't actually zebras (false-positives), and missed detections of zebras (false-negatives).  The detection regions were annotated using a click-and-drag interface, which is a very common interaction design for computer interfaces and a natural motion for detection region annotation.  The detection regions had 8 control points for resizing the detection region and also allowed for moving, rotating, and deleting.  IBEIS automatically gave a species to the detection region, but could be later corrected by a reviewer.  An example image being reviewed in the web interface for appropriate detection regions can be seen in Figure \ref{fig:turking_interface_detection}.  After the image detections were reviewed, a second review procedure was started in order to increase the accuracy of animal identification by adding quality and viewpoint information to the reviewed annotation.

For detection regions, the optimal scenario was to put a bounding box around the entire animal in the frame of the image.  However, due to occlusion from the natural landscape, occlusion from other animals, or poorly framed animals (as seen in Figure \ref{fig:qualities} (b)) sometimes the detection region could not accommodate a bounding box around the entire individual.  For these sightings, a best-fit procedure was followed: try to maximize coverage of the entire animal but prevent significant brush or other animals from being seen inside the bounding box.  The guiding principle when selecting a representative bounding box around an individual was to give the IBEIS identification system as much detail as possible about the foreground, target individual and minimize the amount of distraction from brush or other animals.  In practice, this meant that the resulting annotations fluctuated wildly in pose, viewpoint, size, coverage, and usable detail.  However, as long as identifying marks could be seen in the detection region, the identification system should have been able to find a corresponding match in the database.

\begin{figure}[t]%
%\begin{figure}[!htb]%
	\centering
	\subfloat {{ $\vcenter{\hbox{ \includegraphics[width=0.25\textwidth]{resources/guideline_inverted.jpg} }}$ }}%
    	\caption[The Viewpoint Orientation Bins Allowed by IBEIS]{\textbf{The viewpoint orientation bins allowed by IBEIS.}  The different orientations allowed by IBEIS (0-360) binned into the 8 cardinal and sub-cardinal viewpoints.  The reference point for annotating viewpoint is based on the location of the camera in relation to the animal.  For example, if the animal is facing towards the camera (head looking into the camera), then the correct viewpoint annotation is down; conversely, if the animal is facing away and to the right, then the correct viewpoint annotation is up and to the right.}
    	\label{fig:viewpoint}
\end{figure}

\section{Viewpoint \& Quality}
The need to annotate the viewpoint of the animal was required to facilitate more accurate appearance-based identification results by filtering queried images and the corresponding matches.  As mentioned in Chapter \ref{sec:collection}, plains zebras (and Masai giraffes) are not left-right symmetric.  Thus, IBEIS could only identify animals taken from a consistent viewpoint, which, for the purposes of the GZGC, was the left viewpoint.  The viewpoint annotation on the images allowed IBEIS to not only limit the input to the identification pipeline as only being left-viewpoint images, but it also allowed us to filter the returned matches from IBEIS to have agreeing viewpoints.

Viewpoint information was added to the detected animal as a discretized integer from 0-360 degrees (where 0 degrees signified a left viewpoint) and then binned into the following eight 45-degree semantic viewpoints: \textit{left} (0), \textit{front-left} (45), \textit{front} (90), \textit{front-right} (135), \textit{right} (180), \textit{back-right} (225), \textit{back} (270), and \textit{back-left} (315).  An example image reviewed in the web interface to be given a viewpoint can be seen in Figure \ref{fig:turking_interface_viewpoint}.  The reviewer also had the option of deleting the detection or rotating the detected image by 90-degrees clockwise and counter-clockwise, to correct for bad EXIF rotation parsing during image retrieval.  The viewpoint was oriented based on where the camera was in relation to the animal when the picture was taken, see Figure \ref{fig:viewpoint} for reference.  

\begin{figure}[t]%
%\begin{figure}[!htb]%
	\centering
	\subfloat {{ $\vcenter{\hbox{ \includegraphics[width=0.75\textwidth]{resources/turk_viewpoint.png} }}$ }}%
    	\caption[The Viewpoint Turking Web Interface]{\textbf{The viewpoint turking web interface.}  The web interface allowed multiple reviewers to add viewpoint annotations to an image through a ditributed, highly-parallelized annotation system}
    	\label{fig:turking_interface_viewpoint}
\end{figure}

The viewpoint of an animal was also useful information for building a robust image recognition database.  As more images are added to the database for a particular individual, there builds a degree of diminishing returns in terms of recognizability.  Furthermore, adding too many pictures of an individual can cause unnecessary imbalance to the search structure and can thus lead to poor generalized recognition results.  With viewpoint annotation added to the individual's images, it begins to solve this problem by allowing the identification algorithm to prune semantically duplicate images from the search structure, which frees up representational power for other individuals.  In practice, IBEIS typically limited the number of images to 2 for each of the eight viewpoints, for a maximum total of 16 images for an individual.

\begin{figure}[t]%
%\begin{figure}[!htb]%
	\centering
	\subfloat {{ $\vcenter{\hbox{ \includegraphics[width=0.75\textwidth]{resources/turk_quality.png} }}$ }}%
    	\caption[The Quality Turking Web Interface]{\textbf{The quality turking web interface.}  The web interface allowed multiple reviewers to add quality scores to an image through a distributed, highly-parallelized annotation system}
    	\label{fig:turking_interface_quality}
\end{figure}

%\begin{figure*}[t]%
\begin{figure}[!htb]%
    \centering
    \subfloat[junk]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-junk1.jpeg} }}$ }}%
    \subfloat[junk]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-junk2.jpeg} }}$ }}%
    \subfloat[junk]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-junk3.jpeg} }}$ }}%
    \vspace{0.1cm}
    \subfloat[poor]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-poor1.jpeg} }}$ }}%
    \subfloat[poor]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-poor2.jpeg} }}$ }}%
    \subfloat[poor]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-poor3.jpeg} }}$ }}%
    \vspace{0.1cm}
    \hline
    \vspace{0.1cm}
    \subfloat[ok]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-ok.jpeg} }}$ }}%
    \subfloat[good]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-good.jpeg} }}$ }}%
    \subfloat[excellent]{{ $\vcenter{\hbox{ \includegraphics[width=0.22\textwidth]{resources/quality/quality-excellent.jpeg} }}$ }}%
    \vspace{0.1cm}
    \caption[Example Assigned Quality Scores for Images of Zebras]{\textbf{Example assigned quality scores for images of zebras}.  The assigned quality scores for 9 images of zebras.  The \textit{junk} images (top row) have either extreme occlusion due to brush or due to only a small portion of the animal being photographed.  The \textit{poor} images (middle row), while also having occlusion issues, offer at least some usable information for identification, unlike the \textit{junk} images.  The final three quality metrics (bottom row) are assigned based on various levels of ideal pose, lighting conditions, and clarity.}
        \label{fig:qualities}
\end{figure}

Similar to viewpoint, an image was annotated with a quality score.  The quality score was designed to eliminate images that were poor due to being blurry, having bad illumination, the animal being too occluded by natural landscape or other animals, the animal being cut off for not being captured completely in the image, etc.  Ultimately, the quality score was also used to filter out images, which would have likely been noisy or otherwise poor for identification because the description extracted on these degraded images would have also been poor.  We wanted to maximize the chance of identifying an individual by only putting quality descriptors into the database, which only came from quality images.  An image was assigned a quality score of 1-5 stars -- \textit{junk}, \textit{poor}, \textit{ok}, \textit{good}, and \textit{excellent}, respectively -- with 1 being the lowest quality and 5 being the highest quality.  Examples of images with their assigned qualities can be seen in Figure \ref{fig:qualities}.  Looking at the Figure, it is not easy to cleanly separate the examples for each quality score as it is largely subjective between reviewers.  Nevertheless, the quality was a consistent enough metric to discard poor images for processing; for the GZGC, we processed all images that scored a quality of 3 (\textit{ok}) or higher.  During viewpoint annotation, the reviewer also had the option of setting an image as being marked as \textit{junk}, as a convenience.  During quality annotation, the reviewer also had the option of deleting the detection altogether.  An example image being reviewed in the web interface to be given a quality can be seen in Figure \ref{fig:turking_interface_quality}.

The quality metadata was also used to select the best images for an individual to be used in the recognition search structure.  As a result, viewpoint and quality share a similar utility of providing some form of cleanliness to the database.  %  It should be noted that no images of an individual were completely discarded, but instead were selectively included or ignored during the search database construction

\section{Identification} \label{sec:identification}
The IBEIS prototype used to recognize and identify sightings of animals was used during the GZGC analysis, but is not a product of the current research of the author.  The identification algorithms, interfaces for identification and name conflict resolution, and computer vision techniques described in this section were developed by Jonathan Crall.  This section should function as a summary of the IBEIS identification procedure.  Future Ph.D.\ dissertations will expand on the technical and algorithmic details of the software that powered the identification analysis.

Using viewpoint and quality as a filter, the detected image regions were passed into the IBEIS identification pipeline in order to assign names to the animals.  The images were compared to a database of individuals and the top ranking matches were returned for review.  A reviewer had the opportunity to select none, one, some, or all of the top matches in order to assign names and resolve naming conflicts.  At which point, an animal was then either assigned a new name and recorded in the database or assigned an existing name and it's metadata was associated with that individual's sighting.  

As a secondary identification procedure, we performed identification on the database against itself to identify mistakes made during naming.  The two scenarios that we encountered were: a name \textit{merge} and a name \textit{split}.  Name merges appeared as a single individual appearing in the database under two (or more) different names.  The resolution for merged names was done as new sightings of individuals were added to the database.  Merging names was an integral and common part of the workflow as new sightings could offer new evidence or viewpoints that make merging the two names definitive.  However, not being careful in merging names for sightings lead to having to perform merge splits, which was a more demanding process.

Conversely, a name split appeared as two (or more) individuals appearing in the database under the same name.  Name splits were difficult to identify in the database as the computational requirement to perform a comprehensive check was expensive.  In practice, name splits were investigated as animals with abnormally high number of sightings, which suggested that more than one individual was assigned into the same name.  For these animals, spot checking was sufficient to verify inconsistencies.  Once an inconsistency was identified, the assigned names were simply dropped for all sightings and identification for those images was redone.

\begin{figure}[t]%
%\begin{figure}[!htb]%
	\centering
	\subfloat {{ $\vcenter{\hbox{ \includegraphics[width=0.75\textwidth]{resources/turk_metadata.png} }}$ }}%
    	\caption[The Age and Sex Turking Web Interface]{\textbf{The age and sex turking web interface.}  The web interface allowed multiple reviewers to add age and sex annotations to an image through a ditributed, highly-parallelized annotation system.  Note that this interface was used after names had been assigned to each sighting of an individual.  As a result, multiple images of an individual could be offered to improve the accuracy of aging and sexing.}
    	\label{fig:turking_interface_metadata}
\end{figure}

\section{Age \& Sex}
To be able to analyze population demographics, we provided an interface for a reviewer to annotate the age and sex (when possible) for individuals.  The interface for adding age and sex was more or less the same as viewpoint and quality, but had the advantage of giving the reviewer multiple images of the same \textit{named individual} instead of a single image from a single sighting of an individual.  The advantage of being able to view multiple images of the same individual made the challenge of sexing the individual easier because multiple instances could be used to cross-reference for anatomical or behavioral traits of each sex.  Each individual zebra was given a sex of \textit{female}, \textit{male}, or \textit{unknown} and an age (in months) of 0-3 (\textit{infant}), 3-6 (\textit{infant}), 6-12 (\textit{infant}), 12-24 (\textit{yearling}), 24-36 (\textit{2-year-old}), or 36+ (\textit{adult}).  Figure 
\ref{fig:turking_interface_metadata} shows the turking interface for aging and sexing an individual.

For the GZGC, we were able assign an age to the individuals because all images were collected over a period of only two weeks.  While sexing will always be done for an individual (applied to all images), the aging procedure would eventually have to be done on sightings only (each image at a time).  As the database matures over time, the age of an identified animal can be automatically inferred and the average life-expectancy for the zebra population can be estimated.

