%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER FIVE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{Discussion} \label{sec:discussion}

\section{Issues with the Proposed Method}

\begin{figure*}[t]%
\centering
\includegraphics[width=1\textwidth]{../images/results/bc_dtw_gtrank_hist.png}
\caption{\textbf{Histogram of Ground-Truth Ranks}. Note that the histogram ranges are uneven to better show the lower end of the range. In order to have all matches found within the top-$k$ matches we would have to set $k = 414$.}
\label{fig:gtrank_hist}
\end{figure*}

The primary issue that we encountered is that whether or not a given pair of trailing edges belongs to the same individual based solely on the distance between them is difficult (see Figure \ref{fig:score_sep}).
This hinders finding a threshold distance such that below it we can consider a pair of trailing edges to match (i.e.\ a verification task).
A side effect of this is that small changes in the trailing edge can lead to matching failures, and in the worst case ``push down'' the correct match's rank significantly.
This affects the usability of our method, as ideally a human operator verifying a match would only have to look at the top-$k$ flukes for some small value $k$.
However, we find that in many failure cases the correct match is far down the list, and there is no reasonable value $k$ (i.e.\ that a human operator would want to find a match in) that all matches are within (see Figure \ref{fig:gtrank_hist}). 

While this handling of failure cases is sub-optimal, Figure \ref{fig:score_diff1st2nd} shows that by thresholding the difference in scores between the first and second ranked flukes for a given query, we can determine with high confidence whether or not a given query is matched correctly.
However, we cannot easily ascertain a failure case from this, as there are many success cases that overlap with score differences that characterize failure cases.


There are several other issues with our method, primarily having to do with the stability and generalization capability of the convolutional networks used for fluke keypoint predictions and trailing edge scores.
For the former network, ideally the keypoints could be predicted without needing our assumption on fluke position and orientation, although it is clear to us that this does not happen.
We believe that this is primarily due to the lack of training data that defies these assumptions, although it is also possible that it is beyond the capacity of the models we trained.
That said, having the fluke horizontally oriented and flat to the camera does not only help the keypoint extraction, but also avoids an obscured trailing edge due to out-of-plane rotations (as seen in Figures \ref{fig:example_kp_failure} and \ref{fig:unclear_te}) --- so these assumptions that we make help get a good trailing edge.
However, imposing this requirement complicates and restricts the actual photography of these flukes by requiring the photographer to be in a very specific position with respect to the subject fluke.


We noted that for the trailing edge extractor, it would often use the gradient information to define a trailing edge rather than some deeper semantic image properties (see Figure \ref{fig:example_te_res_failure}).
Part of the reason for this is similar in nature to the problem for training the fluke keypoint network, i.e.\ that a large majority of the dataset represent ``easy'' cases (where the gradient signal correlates with the trailing edge), making it hard to train the network to handle hard cases.
We did use data augmentation to help rectify this bias, but it was not completely effective.
It is possible that more sophisticated data augmentation could help, but primarily we think that spending more time annotating and creating a trailing edge scoring dataset would be ideal.

\section{Future work}

\begin{figure*}[t]%
\centering
\subfloat[][] {
	\includegraphics[width=0.5\textwidth]{../images/results/score_diff_fail_hist.png}
}
\subfloat[][] {
	\includegraphics[width=0.5\textwidth]{../images/results/score_diff_succ_hist.png}
}
\caption{\textbf{Histogram of Score Differences between 1st and 2nd Rank}. Note that the histogram ranges are uneven to better show the higher	end of the range.}
\label{fig:score_diff1st2nd}
\end{figure*}



There is a lot of work that can be done based on this method.
The immediate focus is to achieve the theoretical 93\% accuracy by accurately choosing to use either Hotspotter or our method for any given query and resultant ranking over possible matches.
This would result in a more general method that could be robust to obscured trailing edges and some out-of-plane rotations, while also handling untextured flukes.

One part of this identification pipeline that we mostly ignored is a detection and orientation step.
While orientation is not necessarily important for the curvature measure (as it is rotation invariant), it is important for the trailing edge extraction.
Additionally, being able to detect and crop the fluke automatically from an image would give a much more robust and flexible system.
We did not explore this as most of the images in the Flukebook dataset came pre-cropped around the fluke, as well as rotated such that the major axis of the trailing edge was horizontal.
These conditions both obviate the need for such a system and make it difficult to train a detection and orientation model.

Extracting the trailing edge is currently done with an unsophisticated and restricted algorithm, and improving it is a viable avenue for future work. 
There are several drawbacks, but its inability to handle larger ``slopes'' in the trailing edge without introducing noise is one of the main issues.
Additionally, it is currently non-trivial to efficiently determine the second or third best trailing edge given the completed cost matrix.
This would give us the ability to evaluate candidate trailing edges and choose the best one for matching.
On top of that, the trailing edge scoring networks could be better trained with more annotated trailing edges, the creation of which requires significant manual effort.

There is a lot to be done with the matching algorithm itself, namely in ensuring that it is more tolerant to insignificant deformations in the trailing edge.
One major paradigm that we do not explore in this work is that of extracting and matching multiple features per trailing edge, much in the same way that Hotspotter and Hughes et al.\ \cite{hughes2015automated} operate.
Another avenue of exploration is speeding up our method, either by pruning completely unlikely candidates for a query or by developing an indexing algorithm for an efficient (i.e.\ sub-linear) search of possible matches.
The latter is a difficult task as our distance function is non-metric.
This was beyond the scope of this work, however it would certainly be helpful as the size of datasets on which this method is evaluated increases.

We also believe that the embedding network approach that was briefly described in section \ref{sec:embedding} showed promise and is worth re-evaluating on a larger dataset.

\section{Conclusion}

In this thesis we have presented a novel, fully-automated method for photo-identifying humpback flukes that achieves a high top-1 ranking accuracy on a relatively large dataset.
This method extracts fine grained trailing edge contours from images of flukes and identifies individuals based on sequence properties of the contour curvature.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
